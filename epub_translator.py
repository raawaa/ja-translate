#!/usr/bin/env python3
"""
iFlow CLI æ—¥æ–‡ EPUB ç¿»è¯‘å™¨ï¼ˆä¸¥æ ¼éµå¾ªç”¨æˆ· workflowï¼‰
ä½œè€…ï¼šQwen + ç”¨æˆ·è§„èŒƒ
"""

import asyncio
import json
import os
import re
import time
from pathlib import Path
from typing import List, Dict, Optional
from iflow_sdk import IFlowClient, AssistantMessage, TaskFinishMessage, TimeoutError as SDKTimeoutError, ToolCallMessage, PlanMessage

# ======================
# é…ç½®åŒºï¼ˆæŒ‰éœ€ä¿®æ”¹ï¼‰
# ======================
SOURCE_ROOT = Path("source")
TRANSLATED_ROOT = Path("translated")
SOURCE_OEBPS = SOURCE_ROOT / "OEBPS"  # ç”¨äºå‘åå…¼å®¹
SOURCE_DIR = SOURCE_OEBPS  # å‘åå…¼å®¹ï¼šæŒ‡å‘ source/OEBPS
TRANSLATED_DIR = TRANSLATED_ROOT  # å‘åå…¼å®¹ï¼šæŒ‡å‘ translated
TEMP_DIR = Path("temp")  # è¿‡ç¨‹æ€§æ–‡ä»¶å­˜æ”¾ç›®å½•
CHECKLIST_FILE = TEMP_DIR / "translate-checklist.md"
GLOSSARY_FILE = "glossary.md"  # æœ¯è¯­è¡¨ä¿æŒåœ¨æ ¹ç›®å½•
PROGRESS_FILE = TEMP_DIR / "paragraph_progress.json"
ERROR_LOG_FILE = TEMP_DIR / "error_log.json"
NEW_TERMS_FILE = TEMP_DIR / "new_terms.json"

MAX_RETRY = 3
TIMEOUT_SEC = 60.0
QUALITY_CHECK_INTERVAL = 5

# ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
TRANSLATED_ROOT.mkdir(exist_ok=True)
TEMP_DIR.mkdir(exist_ok=True)

# ======================
# è¾…åŠ©å‡½æ•°
# ======================

def load_glossary() -> Dict[str, str]:
    """åŠ è½½æœ¯è¯­è¡¨ï¼šæ—¥æ–‡ -> ä¸­æ–‡"""
    if not os.path.exists(GLOSSARY_FILE):
        return {}
    glossary = {}
    with open(GLOSSARY_FILE, 'r', encoding='utf-8') as f:
        lines = f.readlines()
        for line in lines[2:]:  # è·³è¿‡æ ‡é¢˜è¡Œ
            if '|' in line:
                parts = line.strip().split('|')
                if len(parts) >= 3:
                    ja = parts[1].strip()
                    zh = parts[2].strip()
                    if ja and zh:
                        glossary[ja] = zh
    return glossary

def save_json(data, filepath: str):
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def load_json(filepath: str, default):
    if os.path.exists(filepath):
        with open(filepath, 'r', encoding='utf-8') as f:
            return json.load(f)
    return default

def should_translate_file(file_path: Path) -> bool:
    """åˆ¤æ–­æ–‡ä»¶æ˜¯å¦éœ€è¦ç¿»è¯‘"""
    # åªç¿»è¯‘ OEBPS ç›®å½•ä¸‹çš„ç‰¹å®šæ–‡æœ¬æ–‡ä»¶
    if "OEBPS" not in file_path.parts:
        return False
    
    ext = file_path.suffix.lower()
    return ext in ['.html', '.xhtml', '.ncx', '.opf']

def extract_translatable_blocks(html: str) -> List[str]:
    """æå–æ‰€æœ‰å¯ç¿»è¯‘çš„ HTML å—ï¼ˆä¿ç•™æ ‡ç­¾ï¼‰"""
    # ä½¿ç”¨ BeautifulSoup ä»¥æ›´å®‰å…¨åœ°è§£æ HTMLï¼Œé¿å…æ­£åˆ™è¡¨è¾¾å¼å¤„ç†å¤æ‚ HTML æ—¶çš„é—®é¢˜
    try:
        from bs4 import BeautifulSoup
        soup = BeautifulSoup(html, 'html.parser')
        elements = soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'div'])
        return [str(elem) for elem in elements]
    except ImportError:
        # å¦‚æœæ²¡æœ‰å®‰è£…BeautifulSoupï¼Œåˆ™ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ä½œä¸ºå¤‡é€‰æ–¹æ¡ˆ
        # åŒ¹é… <p>, <h1>-<h6>, <div>ï¼ˆå¸¦ class çš„å¸¸è§æ­£æ–‡å®¹å™¨ï¼‰
        pattern = r'(<(p|h[1-6]|div)(?:\s[^>]*)?>.*?</\2>)'
        matches = re.findall(pattern, html, re.DOTALL | re.IGNORECASE)
        return [m[0] for m in matches]

def get_file_type(filename: str) -> str:
    """
    æ ¹æ®æ–‡ä»¶æ‰©å±•åç¡®å®šæ–‡ä»¶ç±»å‹
    """
    if filename.endswith('.html') or filename.endswith('.xhtml'):
        return 'html'
    elif filename.endswith('.ncx'):
        return 'ncx'
    elif filename.endswith('.opf'):
        return 'opf'
    else:
        return 'other'

def build_context(blocks: List[str], idx: int) -> tuple:
    prev_block = blocks[idx - 1] if idx > 0 else ""
    curr_block = blocks[idx]
    next_block = blocks[idx + 1] if idx < len(blocks) - 1 else ""
    return prev_block, curr_block, next_block

def extract_translatable_blocks_ncx(content: str) -> List[str]:
    """æå–NCXæ–‡ä»¶ä¸­çš„å¯ç¿»è¯‘æ–‡æœ¬ï¼ˆç« èŠ‚æ ‡é¢˜ç­‰ï¼‰"""
    try:
        from xml.etree import ElementTree as ET
        root = ET.fromstring(content)
        
        # å®šä¹‰å‘½åç©ºé—´
        namespaces = {
            'ncx': 'http://www.daisy.org/z3986/2005/ncx/'
        }
        
        translatable_blocks = []
        
        # æå–æ‰€æœ‰<navLabel><text>ä¸­çš„å†…å®¹
        for nav_point in root.findall('.//ncx:navLabel', namespaces):
            text_elem = nav_point.find('ncx:text', namespaces)
            if text_elem is not None and text_elem.text and contains_japanese(text_elem.text or ""):
                # åŒ…å«å®Œæ•´çš„æ ‡ç­¾ç»“æ„ä»¥ä¾¿æ­£ç¡®æ›¿æ¢
                block = f"<text>{text_elem.text}</text>"
                translatable_blocks.append(block)
        
        return translatable_blocks
    except ET.ParseError as e:
        print(f"è§£æNCXæ–‡ä»¶æ—¶å‡ºé”™: {e}")
        # å¦‚æœè§£æå¤±è´¥ï¼Œå°è¯•ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼
        import re
        matches = re.findall(r'<text>([^<]*)</text>', content)
        blocks = []
        for match in matches:
            if contains_japanese(match):
                blocks.append(f"<text>{match}</text>")
        return blocks
    except Exception as e:
        print(f"å¤„ç†NCXæ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return []

def extract_translatable_blocks_opf(content: str) -> List[str]:
    """æå–OPFæ–‡ä»¶ä¸­çš„å¯ç¿»è¯‘å…ƒæ•°æ®"""
    try:
        from xml.etree import ElementTree as ET
        root = ET.fromstring(content)
        
        # å®šä¹‰å‘½åç©ºé—´
        namespaces = {
            'opf': 'http://www.idpf.org/2007/opf',
            'dc': 'http://purl.org/dc/elements/1.1/'
        }
        
        translatable_blocks = []
        
        # æå–æ‰€æœ‰å¯èƒ½åŒ…å«æ—¥æ–‡çš„å…ƒç´ 
        elements_to_check = [
            'dc:title', 'dc:creator', 'dc:subject', 
            'dc:description', 'dc:publisher', 'dc:contributor'
        ]
        
        for elem_name in elements_to_check:
            for elem in root.findall(f'.//{elem_name}', namespaces):
                if elem.text and contains_japanese(elem.text):
                    # ä¿ç•™æ ‡ç­¾ç»“æ„ï¼Œä¾¿äºåç»­æ›¿æ¢
                    tag_name = elem_name.split(':')[-1]  # è·å–æ ‡ç­¾åï¼ˆå»æ‰å‘½åç©ºé—´å‰ç¼€ï¼‰
                    block = f"<{tag_name}>{elem.text}</{tag_name}>"
                    translatable_blocks.append(block)
        
        return translatable_blocks
    except ET.ParseError as e:
        print(f"è§£æOPFæ–‡ä»¶æ—¶å‡ºé”™: {e}")
        # å¤‡é€‰æ–¹æ¡ˆï¼šä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼
        import re
        matches = re.findall(r'<(?:dc:)?(title|creator|subject|description|publisher|contributor)>([^<]*)</(?:dc:)?\1>', content)
        blocks = []
        for tag, content in matches:
            if contains_japanese(content):
                blocks.append(f"<{tag}>{content}</{tag}>")
        return blocks
    except Exception as e:
        print(f"å¤„ç†OPFæ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return []

def contains_japanese(text: str) -> bool:
    return bool(re.search(r'[\u3040-\u309F\u30A0-\u30FF\u3400-\u4DBF\u4E00-\u9FFF]', text))

def check_chinese_punctuation(text: str) -> bool:
    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ä¸­æ–‡æ ‡ç‚¹ï¼ˆç®€å•è§„åˆ™ï¼‰
    jp_punct = 'ã€‚ã€ãƒ»ã€Œã€ã€ã€ã€ã€‘ï¼ï¼Ÿ'
    for p in jp_punct:
        if p in text:
            return False
    return True

def update_checklist(file_list: List[str], completed_files: set):
    """æ›´æ–° translate-checklist.md"""
    content = "# æ—¥æ–‡ä¹¦ç±ç¿»è¯‘è¿›åº¦è¿½è¸ª\n\n"
    
    # æŒ‰ç±»å‹åˆ†ç»„æ–‡ä»¶
    html_files = [f for f in file_list if f.endswith('.html')]
    ncx_files = [f for f in file_list if f.endswith('.ncx')]
    opf_files = [f for f in file_list if f.endswith('.opf')]
    other_files = [f for f in file_list if f not in html_files + ncx_files + opf_files]

    if html_files:
        content += "## HTMLæ–‡ä»¶\n"
        for f in html_files:
            mark = "x" if f in completed_files else " "
            content += f"- [{mark}] {f}\n"
        content += "\n"

    if ncx_files:
        content += "## ç›®å½•æ–‡ä»¶\n"
        for f in ncx_files:
            mark = "x" if f in completed_files else " "
            content += f"- [{mark}] {f}\n"
        content += "\n"

    if opf_files:
        content += "## å…ƒæ•°æ®æ–‡ä»¶\n"
        for f in opf_files:
            mark = "x" if f in completed_files else " "
            content += f"- [{mark}] {f}\n"
        content += "\n"

    if other_files:
        content += "## å…¶ä»–æ–‡ä»¶\n"
        for f in other_files:
            mark = "x" if f in completed_files else " "
            content += f"- [{mark}] {f}\n"
        content += "\n"

    content += "## ç¿»è¯‘è¿›åº¦ç»Ÿè®¡\n"
    total = len(file_list)
    done = len(completed_files)
    percent = done / total * 100 if total > 0 else 0
    content += f"- æ€»æ–‡ä»¶æ•°: {total}ä¸ªæ–‡ä»¶\n"
    content += f"- å·²ç¿»è¯‘: {done}ä¸ª\n"
    content += f"- å¾…ç¿»è¯‘: {total - done}ä¸ª\n"
    content += f"- å®Œæˆåº¦: {percent:.1f}%\n"
    
    with open(CHECKLIST_FILE, 'w', encoding='utf-8') as f:
        f.write(content)

# ======================
# æ ¸å¿ƒç¿»è¯‘å‡½æ•°
# ======================

async def translate_block(
    client: IFlowClient,
    current_block: str,
    prev_block: str = "",
    next_block: str = "",
    glossary: Dict[str, str] = None,
    max_retries: int = MAX_RETRY
) -> str:
    """
    ç¿»è¯‘å•ä¸ª HTML å—
    """
    # æ„å»ºæœ¯è¯­æç¤º
    glossary_text = ""
    if glossary:
        terms = "\n".join([f"{ja} â†’ {zh}" for ja, zh in list(glossary.items())[:10]])  # é™åˆ¶é•¿åº¦
        glossary_text = f"\n\nè¯·ä¼˜å…ˆä½¿ç”¨ä»¥ä¸‹æœ¯è¯­ç¿»è¯‘ï¼š\n{terms}"

    # æ„å»ºä¸Šä¸‹æ–‡ï¼ˆæˆªæ–­é¿å…è¶…é•¿ï¼‰
    context_prompt = ""
    if prev_block or next_block:
        context_parts = []
        if prev_block:
            clean_prev = re.sub(r'<[^>]+>', '', prev_block)[:30]
            context_parts.append(f"å‰ä¸€æ®µï¼š{clean_prev}...")
        if next_block:
            clean_next = re.sub(r'<[^>]+>', '', next_block)[:30]
            context_parts.append(f"åä¸€æ®µï¼š{clean_next}...")
        context_prompt = "ä¸Šä¸‹æ–‡ï¼š" + "ï¼›".join(context_parts)

    prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ—¥ä¸­ç¿»è¯‘ä¸“å®¶ï¼Œå¯¹ä¸¤ç§è¯­è¨€çš„ç»†å¾®å·®åˆ«ã€æ–‡åŒ–èƒŒæ™¯å’Œæƒ¯ç”¨è¡¨è¾¾æœ‰æ·±å…¥äº†è§£ã€‚è¯·ä¸¥æ ¼éµå®ˆï¼š
- ä»…è¾“å‡ºç¿»è¯‘åçš„ HTML æ®µè½ï¼Œä¸è¦ä»»ä½•è§£é‡Šã€æ³¨é‡Šæˆ–é¢å¤–æ–‡æœ¬
- ä¿æŒåŸå§‹ HTML æ ‡ç­¾ä¸å˜
- ä½¿ç”¨ä¸­æ–‡æ ‡ç‚¹ï¼ˆï¼Œã€‚ï¼ï¼Ÿï¼‰
- æ— æ—¥æ–‡å­—ç¬¦æ®‹ç•™
- å‡†ç¡®å¿ å®äºåŸæ–‡å«ä¹‰ï¼ŒåŒæ—¶ç¡®ä¿ä¸­æ–‡è¡¨è¾¾è‡ªç„¶æµç•…ï¼Œç¬¦åˆä¸­æ–‡é˜…è¯»ä¹ æƒ¯
- æ•æ‰åŸæ–‡çš„è¯­æ°”ã€æ–‡åŒ–ç»†å¾®å·®åˆ«å’Œéšå«æ„ä¹‰
- æ³¨æ„å¤„ç†æ•¬è¯­å’Œæ­£å¼ç¨‹åº¦ï¼Œåœ¨ä¸­æ–‡ä¸­é€‚å½“è°ƒæ•´
- è€ƒè™‘å¯èƒ½æ²¡æœ‰ç›´æ¥å¯¹ç­‰è¯çš„æ–‡åŒ–å¼•ç”¨å’Œè¡¨è¾¾{glossary_text}

{context_prompt}

ç°åœ¨ç¿»è¯‘ä»¥ä¸‹æ®µè½ï¼š
{current_block}
"""

    for attempt in range(max_retries):
        try:
            print(f"  ğŸ“‹ å‘é€ç¿»è¯‘è¯·æ±‚ (å°è¯• {attempt+1}/{max_retries})")
            await client.send_message(prompt)
            response = ""
            start_time = time.time()
            message_count = 0
            tool_call_count = 0
            plan_message_count = 0
            current_agent_id = None
            sub_agents = set()

            async for message in client.receive_messages():
                message_count += 1
                
                if isinstance(message, AssistantMessage):
                    # åŠ¨æ€è·å– agent_id
                    if not current_agent_id:
                        current_agent_id = message.agent_id or "default"
                        if message.agent_id:
                            agent_name = str(message.agent_id)
                            print(f"  ğŸ¤– å½“å‰ Agent: {agent_name} (ID: {message.agent_id})")
                        else:
                            print(f"  ğŸ¤– å½“å‰ Agent: é»˜è®¤ç¿»è¯‘ä»£ç†")
                    
                    response += message.chunk.text
                    # é˜²æ­¢æ— é™ç­‰å¾…
                    if time.time() - start_time > TIMEOUT_SEC:
                        raise SDKTimeoutError("ç¿»è¯‘è¶…æ—¶")
                elif isinstance(message, ToolCallMessage):
                    tool_call_count += 1
                    # åŠ¨æ€è·å–å·¥å…·è°ƒç”¨ä¿¡æ¯
                    tool_name = getattr(message, 'label', 'Unknown')
                    tool_id = getattr(message, 'id', 'Unknown')
                    print(f"  ğŸ› ï¸ å·¥å…·è°ƒç”¨ #{tool_call_count}: {tool_name}")
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰ sub agent ä¿¡æ¯
                    if hasattr(message, 'agent_id') and message.agent_id:
                        sub_agents.add(message.agent_id)
                    
                    # é™é»˜å¤„ç†å·¥å…·è°ƒç”¨æ¶ˆæ¯ï¼Œä¸è¾“å‡ºåˆ°ç¿»è¯‘ç»“æœ
                    pass
                elif isinstance(message, PlanMessage):
                    plan_message_count += 1
                    entries_count = len(message.entries) if hasattr(message, 'entries') else 0
                    print(f"  ğŸ“‹ è®¡åˆ’æ¶ˆæ¯ #{plan_message_count}: {entries_count} ä¸ªè®¡åˆ’é¡¹")
                    # é™é»˜å¤„ç†è®¡åˆ’æ¶ˆæ¯ï¼Œä¸è¾“å‡ºåˆ°ç¿»è¯‘ç»“æœ
                    pass
                elif isinstance(message, TaskFinishMessage):
                    print(f"  âœ… ä»»åŠ¡å®Œæˆ (å…±æ¥æ”¶ {message_count} æ¡æ¶ˆæ¯)")
                    if sub_agents:
                        print(f"  ğŸ”„ ä½¿ç”¨çš„ Sub Agents: {', '.join(sub_agents)}")
                    # ä»»åŠ¡å®Œæˆæ¶ˆæ¯ï¼Œä¸è¾“å‡ºåˆ°ç¿»è¯‘ç»“æœ
                    break

            # æ¸…ç†å“åº”ï¼šåªä¿ç•™ HTML å—ï¼ˆç®€å•ç­–ç•¥ï¼‰
            response = response.strip()
            if response.startswith("```") and response.endswith("```"):
                response = "\n".join(response.split("\n")[1:-1])

            # åŸºç¡€éªŒè¯
            if not response or "<" not in response:
                raise ValueError("æ— æ•ˆç¿»è¯‘ç»“æœ")

            print(f"  ğŸ“Š ç¿»è¯‘å®Œæˆ: {len(response)} å­—ç¬¦")
            return response

        except (Exception, asyncio.CancelledError) as e:
            print(f"  âš ï¸ ç¿»è¯‘å¤±è´¥ (å°è¯• {attempt+1}/{max_retries}): {str(e)}")
            if attempt == max_retries - 1:
                return f"<!-- TRANSLATION_FAILED: {current_block} -->"
            await asyncio.sleep(2)

# ======================
# ä¸»æµç¨‹
# ======================

def extract_translatable_blocks_by_type(content: str, file_type: str) -> List[str]:
    """
    æ ¹æ®æ–‡ä»¶ç±»å‹æå–å¯ç¿»è¯‘çš„æ–‡æœ¬å—
    """
    if file_type == 'html':
        return extract_translatable_blocks(content)
    elif file_type == 'ncx':
        return extract_translatable_blocks_ncx(content)
    elif file_type == 'opf':
        return extract_translatable_blocks_opf(content)
    else:
        # å¯¹äºå…¶ä»–ç±»å‹çš„æ–‡ä»¶ï¼Œæš‚æ—¶è¿”å›ç©ºåˆ—è¡¨
        return []

def update_file_content_by_type(original_content: str, file_type: str, original_blocks: List[str], translated_blocks: List[str]) -> str:
    """
    æ ¹æ®ç¿»è¯‘åçš„å—æ›´æ–°åŸå§‹æ–‡ä»¶å†…å®¹
    """
    updated_content = original_content
    
    for i, (orig_block, trans_block) in enumerate(zip(original_blocks, translated_blocks)):
        if file_type == 'html':
            # å¯¹äºHTMLï¼Œç›´æ¥æ›¿æ¢ï¼ˆç¬¬ä¸€æ¬¡åŒ¹é…ï¼‰
            updated_content = updated_content.replace(orig_block, trans_block, 1)
        elif file_type == 'ncx':
            # å¯¹äºNCXï¼Œæå–ç¿»è¯‘åçš„æ–‡æœ¬ï¼Œæ›¿æ¢åŸå§‹çš„textæ ‡ç­¾å†…å®¹
            import re
            # ä»ç¿»è¯‘åçš„å—ä¸­æå–æ–‡æœ¬
            trans_match = re.search(r'<text>(.*?)</text>', trans_block)
            if trans_match:
                trans_text = trans_match.group(1)
                # ä»åŸå§‹å—ä¸­æå–åŸå§‹æ–‡æœ¬
                orig_match = re.search(r'<text>(.*?)</text>', orig_block)
                if orig_match:
                    orig_text = orig_match.group(1)
                    # æ›¿æ¢åŸå§‹å†…å®¹ä¸­çš„å¯¹åº”éƒ¨åˆ†
                    updated_content = updated_content.replace(
                        f"<text>{orig_text}</text>",
                        f"<text>{trans_text}</text>",
                        1
                    )
        elif file_type == 'opf':
            # å¯¹äºOPFï¼Œæå–ç¿»è¯‘åçš„æ–‡æœ¬ï¼Œæ›¿æ¢åŸå§‹çš„æ ‡ç­¾å†…å®¹
            import re
            # è¯†åˆ«æ ‡ç­¾ç±»å‹
            tag_match = re.search(r'<(\w+)>', orig_block)
            if tag_match:
                tag_name = tag_match.group(1)
                # ä»ç¿»è¯‘åçš„å—ä¸­æå–æ–‡æœ¬
                trans_match = re.search(f'<{tag_name}>(.*?)</{tag_name}>', trans_block)
                if trans_match:
                    trans_text = trans_match.group(1)
                    # ä»åŸå§‹å—ä¸­æå–åŸå§‹æ–‡æœ¬
                    orig_match = re.search(f'<{tag_name}>(.*?)</{tag_name}>', orig_block)
                    if orig_match:
                        orig_text = orig_match.group(1)
                        # æ›¿æ¢åŸå§‹å†…å®¹ä¸­çš„å¯¹åº”éƒ¨åˆ†
                        updated_content = updated_content.replace(
                            f"<{tag_name}>{orig_text}</{tag_name}>",
                            f"<{tag_name}>{trans_text}</{tag_name}>",
                            1
                        )
    
    return updated_content

async def main():
    print("ğŸš€ å¯åŠ¨ iFlow EPUB ç¿»è¯‘å™¨ï¼ˆå®Œæ•´EPUBç»“æ„ç¿»è¯‘ï¼‰")
    print("ğŸ“‹ ç¿»è¯‘æ¨¡å¼: ä¸Šä¸‹æ–‡æ„ŸçŸ¥ç¿»è¯‘ï¼Œä¿æŒHTMLç»“æ„")
    print("ğŸ”§ é…ç½®: æœ€å¤§é‡è¯•æ¬¡æ•°={}, è¶…æ—¶æ—¶é—´={}ç§’".format(MAX_RETRY, TIMEOUT_SEC))

    # åŠ è½½çŠ¶æ€
    progress = load_json(PROGRESS_FILE, {})
    error_log = load_json(ERROR_LOG_FILE, {"errors": []})
    new_terms = load_json(NEW_TERMS_FILE, {"discovered_terms": []})
    glossary = load_glossary()
    
    # è¿ç§»ç°æœ‰è¿›åº¦æ•°æ®ï¼šå°†ç®€å•æ–‡ä»¶åé”®è½¬æ¢ä¸ºç›¸å¯¹è·¯å¾„é”®
    if progress:
        new_progress = {}
        for old_key, value in progress.items():
            # å°è¯•åœ¨SOURCE_ROOTä¸‹æŸ¥æ‰¾æ–‡ä»¶
            found = False
            for file_path in SOURCE_ROOT.rglob(old_key):
                if file_path.is_file():
                    rel_path = file_path.relative_to(SOURCE_ROOT)
                    new_progress[str(rel_path)] = value
                    found = True
                    break
            if not found:
                # å¦‚æœæ‰¾ä¸åˆ°ï¼Œå¯èƒ½æ˜¯æ–‡ä»¶ä¸å­˜åœ¨æˆ–è·¯å¾„å·²å˜åŒ–ï¼Œä¸¢å¼ƒè¯¥è¿›åº¦é¡¹
                print(f"âš ï¸  è¿›åº¦æ•°æ®è¿ç§»ï¼šæœªæ‰¾åˆ°æ–‡ä»¶ '{old_key}'ï¼Œä¸¢å¼ƒå…¶è¿›åº¦")
        progress = new_progress
        save_json(progress, PROGRESS_FILE)  # ç«‹å³ä¿å­˜è¿ç§»åçš„æ•°æ®
    
    # è·å–æ‰€æœ‰å¾…ç¿»è¯‘æ–‡ä»¶ï¼ˆé€’å½’éå†æ•´ä¸ªsourceç›®å½•ï¼‰
    all_files = []    # é€’å½’éå†æ•´ä¸ª source/ ç›®å½•æ ‘
    for file_path in SOURCE_ROOT.rglob("*"):
        if file_path.is_file():
            # è·å–ç›¸å¯¹äº SOURCE_ROOT çš„ç›¸å¯¹è·¯å¾„
            rel_path = file_path.relative_to(SOURCE_ROOT)
            all_files.append(str(rel_path))
    
    # æŒ‰å­—æ¯é¡ºåºæ’åºï¼Œä¿è¯å¤„ç†é¡ºåºä¸€è‡´
    all_files.sort()
        
    if not all_files:
        print("âŒ æœªæ‰¾åˆ° source/ ç›®å½•ä¸­çš„æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥è·¯å¾„")
        return
    completed_files = set()

    # åˆå§‹åŒ– checklistï¼ˆæ‰©å±•åçš„é€»è¾‘ï¼‰
    update_checklist(all_files, completed_files)

    async with IFlowClient() as client:
        print("ğŸ”— å·²è¿æ¥åˆ° iFlow æœåŠ¡")
        
        # åŠ¨æ€è·å–å®¢æˆ·ç«¯é…ç½®ä¿¡æ¯
        try:
            # è·å–å®¢æˆ·ç«¯é…ç½®ä¿¡æ¯
            if hasattr(client, 'options') and client.options:
                options = client.options
                url = getattr(options, 'url', 'Unknown')
                timeout = getattr(options, 'timeout', 'Unknown')
                log_level = getattr(options, 'log_level', 'Unknown')
                print(f"ğŸ“Š è¿æ¥é…ç½®: URL={url}, è¶…æ—¶={timeout}s, æ—¥å¿—çº§åˆ«={log_level}")
                
                # æ£€æŸ¥æ˜¯å¦æœ‰ MCP æœåŠ¡å™¨é…ç½®
                if hasattr(options, 'mcp_servers') and options.mcp_servers:
                    print(f"ğŸ”§ MCP æœåŠ¡å™¨: {len(options.mcp_servers)} ä¸ªå·²é…ç½®")
                    for server in options.mcp_servers:
                        server_name = server.get('name', 'Unknown') if isinstance(server, dict) else str(server)
                        print(f"     - {server_name}")
                else:
                    print("ğŸ”§ MCP æœåŠ¡å™¨: æ— é¢å¤–é…ç½®")
            else:
                print("ğŸ“Š é…ç½®ä¿¡æ¯: ä½¿ç”¨é»˜è®¤é…ç½®")
        except Exception as e:
            print(f"ğŸ“Š é…ç½®ä¿¡æ¯: è·å–å¤±è´¥ - {str(e)}")
        
        for filename in all_files:
            file_type = get_file_type(filename)
            print(f"\nğŸ“„ å¤„ç†æ–‡ä»¶: {filename} (ç±»å‹: {file_type})")
            file_key = filename

            # æ„å»ºæºè·¯å¾„å’Œç›®æ ‡è·¯å¾„
            source_path = SOURCE_ROOT / filename
            dest_path = TRANSLATED_ROOT / filename

            # ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨
            dest_path.parent.mkdir(parents=True, exist_ok=True)

            if not source_path.exists():
                print(f"  âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡")
                continue

            # åˆå§‹åŒ–æ–‡ä»¶è¿›åº¦
            if file_key not in progress:
                progress[file_key] = {
                    "type": file_type,
                    "total_blocks": 0,
                    "completed": [],
                    "failed": [],
                    "current_position": 0
                }

            # æ ¹æ®æ–‡ä»¶ç±»å‹å†³å®šå¦‚ä½•å¤„ç†
            if file_type in ['html', 'ncx', 'opf']:
                # å¯ç¿»è¯‘çš„æ–‡æœ¬æ–‡ä»¶
                original_content = source_path.read_text(encoding='utf-8')
                
                # æ ¹æ®æ–‡ä»¶ç±»å‹æå–å¯ç¿»è¯‘å—
                blocks = extract_translatable_blocks_by_type(original_content, file_type)
                progress[file_key]["total_blocks"] = len(blocks)

                # å‡†å¤‡ç›®æ ‡å†…å®¹ï¼ˆåˆå§‹ä¸ºåŸæ–‡ï¼‰
                translated_content = original_content

                # å¦‚æœæœ‰éœ€è¦ç¿»è¯‘çš„å—ï¼Œåˆ™è¿›è¡Œç¿»è¯‘
                if len(blocks) > 0:
                    # ç”¨äºå­˜å‚¨å·²ç¿»è¯‘çš„å—
                    translated_blocks = [""] * len(blocks)

                    # é€å—å¤„ç†
                    for i, block in enumerate(blocks):
                        if i in progress[file_key]["completed"]:
                            print(f"  âœ… è·³è¿‡å·²ç¿»è¯‘å— {i+1}/{len(blocks)}")
                            # å¦‚æœå—å·²ç¿»è¯‘ï¼Œä»æ–‡ä»¶ä¸­æ¢å¤å·²ç¿»è¯‘çš„å—å†…å®¹
                            translated_blocks[i] = block
                            continue

                        print(f"  ğŸ”¤ ç¿»è¯‘å— {i+1}/{len(blocks)}")

                        # å‡†å¤‡ä¸Šä¸‹æ–‡
                        prev_blk, curr_blk, next_blk = build_context(blocks, i)

                        # è°ƒç”¨ç¿»è¯‘
                        translated_block = await translate_block(
                            client, curr_blk, prev_blk, next_blk, glossary
                        )

                        # å­˜å‚¨ç¿»è¯‘åçš„å—
                        translated_blocks[i] = translated_block

                        # æ›´æ–°å®Œæ•´æ–‡ä»¶å†…å®¹
                        translated_content = update_file_content_by_type(
                            original_content, file_type, 
                            blocks[:i+1], translated_blocks[:i+1]
                        )

                        # æ›´æ–°è¿›åº¦
                        progress[file_key]["completed"].append(i)
                        progress[file_key]["current_position"] = i
                        save_json(progress, PROGRESS_FILE)

                        # æ¯5å—ä¿å­˜ä¸€æ¬¡æ–‡ä»¶ + è´¨é‡æ£€æŸ¥
                        if (i + 1) % QUALITY_CHECK_INTERVAL == 0 or i == len(blocks) - 1:
                            # ä¿å­˜æ–‡ä»¶
                            dest_path.write_text(translated_content, encoding='utf-8')

                            # è´¨é‡æ£€æŸ¥
                            if contains_japanese(translated_block):
                                err_msg = f"å— {i} ä»å«æ—¥æ–‡å­—ç¬¦"
                                print(f"  âŒ {err_msg}")
                                error_log["errors"].append({
                                    "file": filename,
                                    "block": i,
                                    "error": err_msg,
                                    "content": translated_block
                                })
                                save_json(error_log, ERROR_LOG_FILE)

                            if not check_chinese_punctuation(translated_block):
                                print(f"  âš ï¸ å— {i} å¯èƒ½ä½¿ç”¨äº†æ—¥æ–‡æ ‡ç‚¹")

                            print(f"  ğŸ’¾ å·²ä¿å­˜ {filename}ï¼ˆè¿›åº¦ {i+1}/{len(blocks)}ï¼‰")
                else:
                    print(f"  â„¹ï¸ æ–‡ä»¶ä¸­æ²¡æœ‰éœ€è¦ç¿»è¯‘çš„å†…å®¹: {filename}")
                    # ä»ç„¶ä¿å­˜æ–‡ä»¶
                    dest_path.write_text(original_content, encoding='utf-8')
            else:
                # éæ–‡æœ¬æ–‡ä»¶ï¼ˆå¦‚å›¾ç‰‡ã€CSSç­‰ï¼‰ï¼Œç›´æ¥å¤åˆ¶
                print(f"  ğŸ“ å¤åˆ¶éæ–‡æœ¬æ–‡ä»¶: {filename}")
                import shutil
                shutil.copy2(source_path, dest_path)

            # æ–‡ä»¶å®Œæˆ
            completed_files.add(filename)
            update_checklist(all_files, completed_files)
            print(f"âœ… å®Œæˆæ–‡ä»¶: {filename}")

    print("\nğŸ‰ æ‰€æœ‰æ–‡ä»¶å¤„ç†å®Œæ¯•ï¼")
    print(f"è¾“å‡ºç›®å½•: {TRANSLATED_ROOT.absolute()}")

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ ç”¨æˆ·ä¸­æ–­äº†ç¿»è¯‘è¿‡ç¨‹")
        print("âœ… è¿›åº¦å·²ä¿å­˜ï¼Œå¯ä»¥éšæ—¶æ¢å¤")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºå¼‚å¸¸ç»ˆæ­¢: {str(e)}")
        import traceback
        traceback.print_exc()
